{
    "collab_server" : "",
    "contents" : "setwd(\"~/Desktop/gbm-trainR-videos/data/\")\n?gbm\n\nsnp.data<-read.csv(\"~/msk-snp-cdgmp/cdg-data/cdgSNPmatrix-Jinyuan_roy.csv\", sep =\",\", header = T, row.names = 1)\nsnp.by.gene <- split(snp.data, strtrim(rownames(snp.data), 8))\nsnp.by.gene.uniq <- lapply(snp.by.gene, function(x) unique(x))\nsnps<-lapply(snp.by.gene.uniq, function(x) t(x))\nhead(snps)\nsnps\ncdgmp.data<-read.csv(\"cdgTable.csv2\", sep =\",\", header =T)\nm.cdgmp<-tapply(cdgmp.data$logcdg,cdgmp.data$strains,mean )\nnames(m.cdgmp)[16]<-gsub(\"pa_.+_\", \"\", names(m.cdgmp)[16])\nm.cdgmp<-m.cdgmp[rownames(snps[[1]])]\nm.cdgmp\ngene1<-snp.by.gene.uniq[[1]]\nt <- as.matrix(ifelse(m.cdgmp < -1, 0,1 ))\ntg<-t(gene1)\n#library(Matrix)\nmt<-tg#cbind(tg,t)\nsplit<-sample(nrow(mt), floor(0.7*nrow(mt)))\n\ntrain<-mt[split,]\ntest<-mt[-split,]\n\nrequire(gbm)\nrequire(dplyr)\n\ntest<-test[,-ncol(test)]\ntrain<-train[,-ncol(train)]\nend_trn = nrow(train)\nall = rbind(train,test)\nhead(all)\ndim(t)\ndim(all)\nt\ngbm(~X1+X2+X3+X4+X5+X6,         # formula\n    data=data\n    }\nend = nrow(all)\nhead(all)\ncolnames(all)\ngsub(\" \",\",\",noquote(split(colnames(all),\",\")),\" \",\",\")\n?gsub\ndim(all)\ndim(all[1:end_trn,] )\n\nntrees = 5000\nModel = gbm.fit( \n  x = all#[1:end_trn,] #dataframe of features\n  , y = t #dependent variable\n  #two ways to fit the model\n  #use gbm.fit if you are going to specify x = and y = \n  #instead of using a formula\n  #if there are lots of features, I think it's easier to specify \n  #x and y instead of using a formula\n  \n  \n  , distribution = \"bernoulli\"\n  #use bernoulli for binary outcomes\n  #other values are \"gaussian\" for GBM regression \n  #or \"adaboost\"\n  \n  \n  , n.trees = ntrees\n  #Choose this value to be large, then we will prune the\n  #tree after running the model\n  \n  \n  , shrinkage = 0.01 \n  #smaller values of shrinkage typically give slightly better performance\n  #the cost is that the model takes longer to run for smaller values\n  \n  \n  , interaction.depth = 3\n  #use cross validation to choose interaction depth!!\n  \n  \n  , n.minobsinnode = 10\n  #n.minobsinnode has an important effect on overfitting!\n  #decreasing this parameter increases the in-sample fit, \n  #but can result in overfitting\n  \n  , nTrain = round(end_trn * 0.8)\n  #use this so that you can select the number of trees at the end\n  \n  # , var.monotone = c() \n  #can help with overfitting, will smooth bumpy curves\n  \n  , verbose = TRUE #print the preliminary output\n)  \ndim()\ndim(t)\n\n\n\n\n\n#look at the last model built\n#Relative influence among the variables can be used in variable selection\nsummary(Model)\n#If you see one variable that's much more important than all of the rest,\n#that could be evidence of overfitting.\n\n#optimal number of trees based upon CV\ngbm.perf(Model)\n\n#look at the effects of each variable, does it make sense?\n?plot.gbm\nfor(i in 1:length(Model$var.names)){\n  plot(Model, i.var = i\n       , ntrees = gbm.perf(Model, plot.it = FALSE) #optimal number of trees\n       , type = \"response\" #to get fitted probabilities\n  )\n}\n\n###########################################\n\n\n\n\n\n\n\n\n\n################ Make predictions ##################\n#test set predictions\nTestPredictions = predict(object = Model,newdata =all[(end_trn+1):end,]\n                          , n.trees = gbm.perf(Model, plot.it = FALSE)\n                          , type = \"response\") #to output a probability\n#training set predictions\nTrainPredictions = predict(object = Model,newdata =all[1:end_trn,]\n                           , n.trees = gbm.perf(Model, plot.it = FALSE)\n                           , type = \"response\")\n\n\n#round the predictions to zero or one\n#in general, don't do this!\n#it was only because the answers in the comp had to be 0 or 1\nTestPredictions = round(TestPredictions)\nTrainPredictions = round(TrainPredictions)\n#could also mess around with different cutoff values\n#would need CV to determine the best\n\n\nhead(TrainPredictions, n = 20)\nhead(survived, n = 20)\n\n\n\n#in sample classification accuracy\n1 - sum(abs(survived - TrainPredictions)) / length(TrainPredictions) \n#depending upon the tuning parameters, \n#I've gotten this as high as 99%, but that model \n#resulted in lower test set scores\n\n\n#to get predicted out of sample accuracy\n#need to set aside a testing data set\n\n\n\n\n\n#write the submission\nsubmission = data.frame(PassengerId = 1:nrow(test), survived = TestPredictions)\nwrite.csv(submission, file = \"gbm submission.csv\", row.names = FALSE)\n#####################################################\n\nN <- 1000\n\nX1 <- runif(N)\nX2 <- 2*runif(N)\nX3 <- ordered(sample(letters[1:4],N,replace=TRUE),levels=letters[4:1])\nX4 <- factor(sample(letters[1:6],N,replace=TRUE))\nX5 <- factor(sample(letters[1:3],N,replace=TRUE))\nX6 <- 3*runif(N) \nmu <- c(-1,0,1,2)[as.numeric(X3)]\n\nSNR <- 10 # signal-to-noise ratio\nY <- X1**1.5 + 2 * (X2**.5) + mu\nsigma <- sqrt(var(Y)/SNR)\nY <- Y + rnorm(N,0,sigma)\n\n# introduce some missing values\nX1[sample(1:N,size=500)] <- NA\nX4[sample(1:N,size=300)] <- NA\ntrain<-tg\ndata <- data.frame(Y=t,X1=train[,1],X2=train[,2],X3=train[,3],X4=train[,4],X5=train[,5],X6= train[,6])\nlength(Y)\nlength(X1)\n# fit initial model\ngbm1 <-\n  gbm(Y~X1+X2+X3+X4+X5+X6,         # formula\n      data=data,                   # dataset\n      var.monotone=c(0,0,0,0,0,0), # -1: monotone decrease,\n      # +1: monotone increase,\n      #  0: no monotone restrictions\n      distribution=\"binomial\",     # see the help for other choices\n      n.trees=1000,                # number of trees\n      shrinkage=0.05,              # shrinkage or learning rate,\n      # 0.001 to 0.1 usually work\n      interaction.depth=3,         # 1: additive model, 2: two-way interactions, etc.\n      bag.fraction = 0.5,          # subsampling fraction, 0.5 is probably best\n      train.fraction = 0.5,        # fraction of data for training,\n      # first train.fraction*N used for training\n      n.minobsinnode = 10,         # minimum total weight needed in each node\n      cv.folds = 3,                # do 3-fold cross-validation\n      keep.data=TRUE,              # keep a copy of the dataset with the object\n      verbose=FALSE,               # don't print out progress\n      n.cores=1)                   # use only a single core (detecting #cores is\n# error-prone, so avoided here)\n\n# check performance using an out-of-bag estimator\n# OOB underestimates the optimal number of iterations\nbest.iter <- gbm.perf(gbm1,method=\"OOB\")\nprint(best.iter)\n\n",
    "created" : 1501400559844.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2684903053",
    "id" : "C93578CD",
    "lastKnownWriteTime" : 1501404664,
    "last_content_update" : 1501404664534,
    "path" : "~/machine_learning/gbm.R",
    "project_path" : null,
    "properties" : {
        "tempName" : "Untitled13"
    },
    "relative_order" : 9,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}