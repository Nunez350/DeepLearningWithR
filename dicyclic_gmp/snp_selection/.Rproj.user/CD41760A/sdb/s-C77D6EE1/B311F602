{
    "collab_server" : "",
    "contents" : "rm(list=ls(all=TRUE))\n\nlibrary(glmnet)\nsource('~/machine_learning/dicyclic_gmp/snp_selection/di_ci_gmp_data.R')\n\nt<-m.cdgmp\n\n#tg<-snp.by.gene.uniq[[49]]\n#x<-tg#for testing purposes\npar(mfrow=c(2,5)) \n\nfit_plots<-function(x,s){\n  lapply(snp.by.gene.uniq, function(x){\n    x<-t(x)\n    i=parent.frame()$i[]\n    split<-sample(nrow(x), floor(0.7*nrow(x)))\n    train.d<-as.matrix(cbind(x,t)[split,])\n    test.d<-as.matrix(cbind(x,t)[-split,])\n    train.m<-train.d[,-ncol(train.d)]\n    target<-train.d[,ncol(train.d)]\n    target<-abs(target)\n    train.m\n    target\n    fit <- glmnet(train.m, target, family = \"poisson\")\n    cv<-cv.glmnet(train.m,target,nfolds = 10)#cross_validate to get the best lambda\n    pred<-predict(fit, train.m, type=\"response\",s=cv$lambda.min)#p\n    plot(fit, label = TRUE) \n    title(paste(\"Gene\",i,\"SNPS\",ncol(x)),line = +2)\n       i<-i+1\n  })\n  \n}\npdf(\"Fitted Models\")\nfit_plots(snp.by.gene.uniq)\ndev.off()\n# Each curve corresponds to a variable. It shows the path of its coefficient against the `1-norm of the whole\n# coefficient vector at as  varies. The axis above indicates the number of nonzero coefficients at the current ,\n# which is the effective degrees of freedom (df ) for the lasso.\n# \ncv_plots<-function(x,s){\n  lapply(snp.by.gene.uniq, function(x){\n    x<-t(x)\n    i=parent.frame()$i[]\n    split<-sample(nrow(x), floor(0.7*nrow(x)))\n    train.d<-as.matrix(cbind(x,t)[split,])\n    test.d<-as.matrix(cbind(x,t)[-split,])\n    train.m<-train.d[,-ncol(train.d)]\n    target<-train.d[,ncol(train.d)]\n    target<-abs(target)\n    fit <- glmnet(train.m, target, family = \"poisson\")\n    cv<-cv.glmnet(train.m,target,nfolds = 10)#cross_validate to get the best lambda\n    pred<-predict(fit, train.m, type=\"response\",s=cv$lambda.min)#p\n    plot(cv, label = TRUE) \n    title(paste(\"Gene\",i,\"SNPS\",ncol(x)),line = +2)\n    i<-i+1\n  })\n  \n}\ncv_plots(snp.by.gene.uniq)\n# It includes the cross-validation curve (red dotted line), and upper and lower standard deviation curves along\n# the  sequence (error bars). Two selected ’s are indicated by the vertical dotted lines (see below).\n# \nfit_lambda<-function(x,s){\n  lapply(snp.by.gene.uniq, function(x){\n    x<-t(x)\n    i=parent.frame()$i[]\n    split<-sample(nrow(x), floor(0.7*nrow(x)))\n    train.d<-as.matrix(cbind(x,t)[split,])\n    test.d<-as.matrix(cbind(x,t)[-split,])\n    train.m<-train.d[,-ncol(train.d)]\n    target<-train.d[,ncol(train.d)]\n    target<-abs(target)\n    fit <- glmnet(train.m, target, family = \"poisson\")\n    cv<-cv.glmnet(train.m,target,nfolds = 10)#cross_validate to get the best lambda\n    pred<-predict(fit, train.m, type=\"response\",s=cv$lambda.min)#p\n    plot(fit, xvar = \"lambda\", label = TRUE)\n    #plot(fit, label = TRUE) \n    title(paste(\"Gene\",i,\"SNPS\",ncol(x)),line = +2)\n    i<-i+1\n  })\n}\nfit_lambda(snp.by.gene.uniq)\n\n\nfit_deviance<-function(x,s){\n  lapply(snp.by.gene.uniq, function(x){\n    x<-t(x)\n    i=parent.frame()$i[]\n    split<-sample(nrow(x), floor(0.7*nrow(x)))\n    train.d<-as.matrix(cbind(x,t)[split,])\n    test.d<-as.matrix(cbind(x,t)[-split,])\n    train.m<-train.d[,-ncol(train.d)]\n    target<-train.d[,ncol(train.d)]\n    target<-abs(target)\n    fit <- glmnet(train.m, target, family = \"poisson\")\n    cv<-cv.glmnet(train.m,target,nfolds = 10)#cross_validate to get the best lambda\n    pred<-predict(fit, train.m, type=\"response\",s=cv$lambda.min)#p\n    plot(fit, xvar = \"dev\", label = TRUE)\n    #plot(fit, label = TRUE) \n    title(paste(\"Gene\",i,\"SNPS\",ncol(x)),line = +2)\n    i<-i+1\n  })\n}\n# fit_deviance(snp.by.gene.uniq)\n# This is percent deviance explained on\n# the training data. What we see here is that toward the end of the path this value are not changing much, but\n# the coefficients are “blowing up” a bit.\n\n\nx<-NULL\n\npar(mfrow=c(2,5)) \ndev.off()\n\nlm_corr_plot<-function(i){\n    x<-t(snp.by.gene.uniq[[i]])\n    split<-sample(nrow(x), floor(0.7*nrow(x)))\n    train.d<-as.matrix(cbind(x,t)[split,])\n    test.d<-as.matrix(cbind(x,t)[-split,])\n    train.m<-train.d[,-ncol(train.d)]\n    target<-train.d[,ncol(train.d)]\n    target<-abs(target)\n    fit <- glmnet(train.m, target, family = \"poisson\")\n    cv<-cv.glmnet(train.m,target,nfolds = 10)#cross_validate to get the best lambda\n    pred<-predict(fit, train.m, type=\"response\",s=cv$lambda.min)\n    plot(pred[,1], target)\n    title(paste(\"Gene\",i,\"SNPS\",ncol(x)),line = +2)\n    #abline(lm( target ~ pred[,1]), col=2)\n    }\n    \npar(mfrow=c(2,5)) \nfor (j in 1:50){\n    lm_corr_plot(j)\n      #g(j)\n    abline(lm( target ~ pred[,1]), col=2)\n    }#summary(lm( target ~ pred[,1]))\n    \n    \n    dev.off()\n\n    #r<-cor.test(pred[,1], target)\n    #r$parameter\n    #r$p.value\n    #r$estimate\n    \n    #plot(fit, label = TRUE) \n    #title(paste(\"Gene\",i,\"SNPS\",ncol(x)),line = +2)\n    i<-i+1\n  })\n  \n}\n\n\nr.corr<-vector()\npvalues<-vector()\n  for (c in 1:50){  \n  x<-t(snp.by.gene.uniq[[c]])\n  split<-sample(nrow(x), floor(0.7*nrow(x)))\n  train.d<-as.matrix(cbind(x,t)[split,])\n  test.d<-as.matrix(cbind(x,t)[-split,])\n  train.m<-train.d[,-ncol(train.d)]\n  target<-train.d[,ncol(train.d)]\n  target<-abs(target)\n  fit <- glmnet(train.m, target, family = \"poisson\")\n  cv<-cv.glmnet(train.m,target,nfolds = 10)#cross_validate to get the best lambda\n  pred<-predict(fit, train.m, type=\"response\",s=cv$lambda.min)\n  r<-cor.test(pred[,1], target)\n  r.corr[[c]]<-r$estimate\n  pvalues[[c]]<-r$p.value\n}\npar(mfrow=c(1,2)) \nhist(r.corr, main= \"R-squared\")\nhist(pvalues, main = \"P-values\")\n\ncor.test(pred[,1], target)\ncollect\ncorr_list<-function(i){\n  \n  x<-t(snp.by.gene.uniq[[i]])\n  split<-sample(nrow(x), floor(0.7*nrow(x)))\n  train.d<-as.matrix(cbind(x,t)[split,])\n  test.d<-as.matrix(cbind(x,t)[-split,])\n  train.m<-train.d[,-ncol(train.d)]\n  target<-train.d[,ncol(train.d)]\n  target<-abs(target)\n  fit <- glmnet(train.m, target, family = \"poisson\")\n  cv<-cv.glmnet(train.m,target,nfolds = 10)#cross_validate to get the best lambda\n  pred<-predict(fit, train.m, type=\"response\",s=cv$lambda.min)\n  r<-cor.test(pred[,1], target)\n  collect<-r$estimate\n}\nrc<-vector()\nfor (c in 1:50){\n  rc[c]<-corr_list(c)\n  \n}\nhist(rc)\n\ncollect\nfit_plots<-function(x){\n  for (j in 1:20){\n  \n    x<-t(x)\n    i=parent.frame()$i[]\n    split<-sample(nrow(x), floor(0.7*nrow(x)))\n    train.d<-as.matrix(cbind(x,t)[split,])\n    test.d<-as.matrix(cbind(x,t)[-split,])\n    train.m<-train.d[,-ncol(train.d)]\n    target<-train.d[,ncol(train.d)]\n    target<-abs(target)\n    fit <- glmnet(train.m, target, family = \"poisson\")\n    cv<-cv.glmnet(train.m,target,nfolds = 10)#cross_validate to get the best lambda\n    pred<-predict(fit, train.m, type=\"response\",s=cv$lambda.min)#p\n    r<-cor.test(pred[,1], target)\n    #plot(fit, label = TRUE) \n    #title(paste(\"Gene\",i,\"SNPS\",ncol(x)),line = +2)\n    #i<-i+1\n  }\nr  \n  \n}\n#r$parameter\n#r$p.value\n#r$estimate\nfit_plots(snp.by.gene.uniq)\n",
    "created" : 1501851763515.000,
    "dirty" : false,
    "encoding" : "",
    "folds" : "11|25|31|0|\n39|24|57|0|\n62|26|80|0|\n84|28|102|0|\n176|23|190|0|\n",
    "hash" : "1802298332",
    "id" : "B311F602",
    "lastKnownWriteTime" : 6,
    "last_content_update" : 1505098965451,
    "path" : null,
    "project_path" : null,
    "properties" : {
        "source_window_id" : "wpep9masew4u9",
        "tempName" : "Untitled4"
    },
    "relative_order" : 14,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}