{
    "collab_server" : "",
    "contents" : "#lasso for binary target\nrm(list=ls(all=TRUE))\nlibrary(glmnet)\nsource('~/machine_learning/dicyclic_gmp/snp_selection/di_ci_gmp_data.R')\nt <- as.matrix(ifelse(m.cdgmp < -1, 0,1 ))\ntg<-t(snp.by.gene.uniq[[49]])#2 dim 3 by 30\nx<-tg#for testing purposes\nx[,25]\ndim(x)\nc.lasso<-function(x){\n  split<-sample(nrow(x), floor(0.7*nrow(x)))\n  train.d<-as.matrix(cbind(x,t)[split,])\n  test.d<-as.matrix(cbind(x,t)[-split,])\n  fit <- glmnet(train.d, train.d[,ncol(train.d)], family = \"binomial\")\n  plot(fit$a0)\n  fit$npasses\n  plot(fit$beta)\n  plot(fit)\n  plot(train.d[,-ncol(train.d)])\n  fit$beta[1,]\n  fit$beta\n  plot(1:100, fit$beta[1,], type = \"l\", ylim=c(-9,8))\n  length(fit$beta[1,])\n  cv<-cv.glmnet(train.m,target,nfolds = 10)#cross_validate to get the best lambda\n  plot(cv.2)\n  plot(cv)\n  \n  # After running cv.glmnet, you don't have to rerun glmnet! Every lambda in the grid (cv$lambda) has already been run. This technique is called \"Warm Start\" and you can read more about it here. Paraphrasing from the introduction, the Warm Start technique reduces running time of iterative methods by using the solution of a different optimization problem (e.g., glmnet with a larger lambda) as the starting value for a later optimization problem (e.g., glmnet with a smaller lambda).\n  summary(cv)\n  plot(fit)  \n  plot(fit, xvar = \"lambda\", label = TRUE)\n  # Let’s plot “fit” against the log-lambda value and with each curve labeled.\n  plot(fit, xvar = \"dev\", label = TRUE)\n  # Now when we plot against %deviance we get a very different picture. This is percent deviance explained on\n  # the training data. What we see here is that toward the end of the path this value are not changing much, but\n  # the coefficients are “blowing up” a bit. This lets us focus attention on the parts of the fit that matter. This\n  # will especially be true for other models, such as logistic regression.\n  # \n  \n  #cv$lambda.1se instead of cv$lambda.min\n  pred<-predict(fit, train.d, type=\"response\",s=cv.2$lambda.min)#predict\n  pred\n  predict.fishnet(fit, train.d, type=\"response\",s=cv.2$lambda.min)#predict\n  print(fit)\n  plot(fit)\n  train.d[,ncol(train.d)]\n  pred==train.d[,ncol(train.d)]\n  round(pred)==train.d[,ncol(train.d)]\n  mod.prediction<-round(pred)==train.d[,ncol(train.d)]\n  length(which(mod.prediction==TRUE))\n  \n  acc<-length(round(pred)==nrow(train.d))/nrow(train.d)*100\nreturn(acc)\n}\nc.lasso(x)\ndim(fit2$beta)\nplot(1:length(o),o$acc1)\nplot(1:100, fit2$beta[1,], type = \"l\", ylim=c(-9,8))\nlout<-list()\npar(mfrow=c(1,1)) \nsummary(fit2)\nlapply(snp.by.gene.uniq, function(x){\n  ac<-c.lasso(t(x))\n  print(ac)\n})\nfit2$beta\nplot(1:90, fit2$beta[1,], type = \"l\", ylim=c(-9,8))\nfit2$beta\nlout\nfit$a0\n\n\n\npred<-predict(fit, train.m, type=\"response\",s=cv.2$lambda.min)#predict\npred\n\ndim(train.m)\nlibrary(SimPhe)\n\nx1 <- rnorm(4000, mean = 5, sd = 10)\nx2 <- rnorm(4000, mean = 10, sd = 30)\nx <- matrix(cbind(x1, x2), ncol = 2)\n# test original correlation\ncor.test(x[, 1], x[, 2])\n# correlation matrix\ncorM <- matrix(c(1, 0.6, 0.6, 1), ncol = 2)\n# standard deviation matrix\nsdM <- matrix(c(10, 0, 0, 30), ncol = 2)\n# build correlation\nx.new <- build.cor.phe(x, corM, sdM)\n# check mean and standard deviation of new data set\napply(x.new, 2, mean)\napply(x.new, 2, sd)\n# test correlation\ncor.test(x.new[, 1], x.new[, 2])\nx.new[, 1]\n\nx1 <- rnorm(4000, mean = 5, sd = 10)\nx2 <- rnorm(4000, mean = 10, sd = 30)\nx <- matrix(cbind(x1, x2), ncol = 2)\nbuild.sd.matrix(x)\n\n\nplot(xx, yy, type = \"l\")  ## plot density curve (or use `plot(d)`)\n\nplot\nplot(fit)\nplot(glmnet(x,y),label = TRUE )\nplot(1:90, fit2$beta[1,], type = \"l\", ylim=c(-9,8))\nlines(1:90, fit2$beta[2,], type = \"l\", col=2)\nlines(1:90, fit2$beta[3,], type = \"l\", col=3)\nlines(1:90, fit2$beta[4,], type = \"l\", col=4)\nlines(1:90, fit2$beta[5,], type = \"l\", col=5)\nlines(1:90, fit2$beta[6,], type = \"l\", col=6)\ndata.matrix(l=fit$lambda, d=deviance(fit))[abs(l-reg.lambda) == min(abs(l-reg.lambda))]$d[1]\nl=fit$lambda\nl\n#d=deviance(fit))[abs(l-reg.lambda) \ndf<-matrix(l,d)\nd=deviance(fit)\ndf[abs(l-reg.lambda) == min(abs(l-reg.lambda))]$d[1]\n\nlibrary(plotmo) # for plotres\nplotres(fit)\npar(mar=c(4.5,4.5,1,4))\nplot(fit)\nvnat=coef(fit)\nvnat=vnat[-1,ncol(vnat)] # remove the intercept, and get the coefficients at the end of the path\nvn=paste(\"var\",1:24)\naxis(4, at=vnat,line=-.5,label=vn,las=1,tick=FALSE, cex.axis=0.5)\nlength(vnat)\nOptions are almost the same as the Gaussian family except that for type.measure, * “deviance” (default)\ngives the deviance * “mse” stands for mean squared error * “mae” is for mean absolut\n\nopt.lam = c(cv.2$lambda.min, cv.2$lambda.1se)\ncoef(cv.2, s = opt.lam)\nglmnet.control()\n#lmnet fits the entire solution path for Lasso or elastic-net problems efficiently with various\ntechniques such as warm starts, which are essential for the nonlinear models like logistic regression",
    "created" : 1501722114956.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "318462317",
    "id" : "3EC18A3A",
    "lastKnownWriteTime" : 1502200520,
    "last_content_update" : 1502200520959,
    "path" : "~/machine_learning/dicyclic_gmp/snp_selection/lasso_binomial_target.R",
    "project_path" : "lasso_binomial_target.R",
    "properties" : {
    },
    "relative_order" : 5,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}